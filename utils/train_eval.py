# utils/train_eval.py
# -*- coding: utf-8 -*-
"""
è®­ç»ƒä¸è¯„ä¼°ï¼ˆNPS/TFF é€šç”¨ï¼Œå¼‚æ„å›¾ + è¾¹å±æ€§ + å¯¹æ¯”/ä¸€è‡´æ€§ï¼‰
- SupCon åŸºäº z_fused
- XMD   åŸºäº z_op / z_dose
- AUG   åŸºäº z_fused çš„è½»é‡ä¸€è‡´æ€§ï¼ˆdropout æˆ–åŠ å™ªï¼‰ï¼Œå— --use_aug, --lmbd_aug æ§åˆ¶
"""

import os, inspect, random, sys
from typing import List, Optional, Tuple
import numpy as np
from collections import Counter

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam, SGD, AdamW, RMSprop, Adagrad
from torch_geometric.loader import DataLoader
from torch.utils.data import Subset
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from torch.amp import autocast, GradScaler
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau
# ---- è§£å†³ç›´æ¥è¿è¡Œæ—¶çš„å¯¼å…¥è·¯å¾„ ----
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# ---- æ•°æ®é›†ä¸æ¨¡å‹ ----
import dataset.data_sampling_lag_edge_attr as ds
from models.temporal_hetero_gnn_edge_attr_contrastive import TemporalPhysicalHeteroGNN_V2


# ==========================
# å·¥å…·å‡½æ•°
# ==========================
def set_seed(seed: int = 42):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)


def class_balanced_weights(labels: List[int], num_classes: int, beta: float = 0.999) -> torch.Tensor:
    cnt = Counter(labels)
    weights = []
    for c in range(num_classes):
        n_c = cnt.get(c, 0)
        if n_c <= 0:
            weights.append(0.0); continue
        w = (1.0 - beta) / (1.0 - (beta ** n_c))
        weights.append(w)
    w = torch.tensor(weights, dtype=torch.float32)
    if w.sum() > 0:
        w = w * (len(w) / (w.sum() + 1e-12))  # å½’ä¸€åˆ°å‡å€¼=1
    return w


class FocalLoss(nn.Module):
    """å¤šåˆ†ç±» Focal Lossï¼ˆå¯¹ CE åŠ æƒï¼‰"""
    def __init__(self, gamma: float = 2.0, weight: Optional[torch.Tensor] = None, reduction: str = "mean"):
        super().__init__()
        self.gamma = gamma
        if weight is not None:
            self.register_buffer("weight_buf", weight)
        else:
            self.weight_buf = None  # type: ignore
        self.reduction = reduction

    def forward(self, logits: torch.Tensor, target: torch.Tensor):
        ce = F.cross_entropy(logits, target, weight=self.weight_buf, reduction="none")
        pt = torch.exp(-ce)
        loss = ((1 - pt) ** self.gamma) * ce
        if self.reduction == "mean": return loss.mean()
        if self.reduction == "sum":  return loss.sum()
        return loss


def supcon_loss(
    z: Optional[torch.Tensor],
    y: torch.Tensor,
    tau: float = 0.1,
    soft: bool = False,
    label_tau: float = 0.5,
) -> torch.Tensor:
    """
    Supervised contrastive loss with optional SoftCL weighting.

    é»˜è®¤ï¼ˆsoft=Falseï¼‰è¡Œä¸ºä¸åŸå®ç°å®Œå…¨ä¸€è‡´ï¼š
    - z å½’ä¸€åŒ–ï¼›
    - ç›¸ä¼¼åº¦ sim = (z z^T) / tauï¼›
    - åŒæ ‡ç­¾ä¸ºæ­£æ ·æœ¬ï¼ˆå»å¯¹è§’çº¿ï¼‰ï¼ŒæŒ‰ log-softmax åå¯¹æ­£æ ·æœ¬å¹³å‡ï¼›

    å½“ soft=Trueï¼š
    - åŸºäºæ ‡ç­¾æ„é€  dist (åŒç±»=0ï¼Œå¼‚ç±»=1)ï¼›
    - soft_pos = exp(-dist / label_tau) å¹¶ç½®é›¶å¯¹è§’çº¿ï¼›
    - å¯¹æ¯è¡Œ soft_pos å½’ä¸€åŒ–ï¼›
    - ä½¿ç”¨ soft_pos å¯¹æ¯è¡Œ log æ¦‚ç‡åŠ æƒæ±‚å’Œï¼ˆSoftCLï¼‰ã€‚

    é²æ£’æ€§ï¼šå½“ z æ— æ•ˆæˆ– batch<2 æ—¶è¿”å› 0ï¼ˆä¸ y åœ¨åŒä¸€è®¾å¤‡ï¼‰ã€‚
    """
    # â€”â€” æ— æ•ˆè¾“å…¥ä¸ batch è¿‡å° â€”â€”
    if z is None or z.numel() == 0:
        return torch.tensor(0.0, device=y.device)
    if z.dim() == 0 or z.size(0) < 2:
        return torch.tensor(0.0, device=y.device)

    # å½’ä¸€åŒ–ä¸ç›¸ä¼¼åº¦
    z = F.normalize(z, dim=-1)
    sim = torch.mm(z, z.t()) / max(tau, 1e-6)

    # ä¸ºäº†ä¸æ—§å®ç°ä¿æŒä¸€è‡´ï¼Œlogits/exp_logits çš„å¤„ç†ä¿æŒä¸å˜
    logits = sim - torch.max(sim, dim=1, keepdim=True).values
    # å»é™¤è‡ªèº«å‚ä¸ï¼šé€šè¿‡ (1 - I) å±è”½å¯¹è§’çº¿
    I = torch.eye(z.size(0), device=z.device)
    exp_logits = torch.exp(logits) * (1 - I)
    log_den = torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-9)
    log_prob = logits - log_den  # æ¯è¡Œ log-softmaxï¼ˆå»æ‰å¯¹è§’çº¿çš„å½’ä¸€åŒ–ï¼‰

    # ç¡¬å¯¹æ¯”ï¼ˆä¸ç°æœ‰å®ç°å®Œå…¨ä¸€è‡´ï¼‰
    if not soft:
        yv = y.view(-1, 1)
        mask_pos = torch.eq(yv, yv.t()).float()
        mask_pos = mask_pos - I  # å»é™¤å¯¹è§’çº¿
        denom = (mask_pos.sum(1) + 1e-9)
        mean_log_prob_pos = (mask_pos * log_prob).sum(1) / denom
        return -mean_log_prob_pos.mean()

    # è½¯å¯¹æ¯”ï¼ˆSoftCLï¼‰
    # è·ç¦»çŸ©é˜µï¼šåŒç±»=0ï¼Œå¼‚ç±»=1
    yv = y.view(-1, 1)
    dist = (yv != yv.t()).float()
    lt = max(float(label_tau), 1e-6)
    soft_pos = torch.exp(-dist / lt)
    # å»é™¤å¯¹è§’çº¿ï¼Œé¿å…è‡ªèº«ä½œâ€œæ­£æ ·æœ¬â€
    soft_pos = soft_pos * (1 - I)
    # è¡Œå½’ä¸€åŒ–ï¼ˆè‹¥å…¨é›¶åˆ™ä¿æŒä¸ºé›¶è¡Œï¼Œåˆ†æ¯åŠ å°å¸¸æ•°é˜²æ­¢é™¤é›¶å‘Šè­¦ï¼‰
    soft_pos = soft_pos / (soft_pos.sum(dim=1, keepdim=True) + 1e-12)
    # ç”¨è½¯æƒé‡åŠ æƒæ¯è¡Œçš„ log æ¦‚ç‡
    weighted_log = (soft_pos * log_prob).sum(1)
    return -weighted_log.mean()


def cosine_align_loss(z_a: Optional[torch.Tensor], z_b: Optional[torch.Tensor]) -> torch.Tensor:
    if z_a is None or z_b is None:
        dev = z_a.device if z_a is not None else (z_b.device if z_b is not None else 'cpu')
        return torch.tensor(0.0, device=dev)
    za = F.normalize(z_a, dim=-1); zb = F.normalize(z_b, dim=-1)
    cos = (za * zb).sum(dim=-1)
    return (1.0 - cos).mean()


def ortho_penalty(z_shared: Optional[torch.Tensor],
                  z_private: Optional[torch.Tensor]) -> torch.Tensor:
    """
    æ­£äº¤çº¦æŸæŸå¤±ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„ shared ä¸ private è¡¨ç¤ºåšä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå–å¹³æ–¹å†å¯¹ batch å¹³å‡ã€‚
    - è¾“å…¥ä»»ä¸€ä¸º None æ—¶è¿”å› 0ï¼ˆè®¾å¤‡ä¸å·²æœ‰å¼ é‡ä¸€è‡´ï¼‰ã€‚
    - ä¸åšç¼©æ”¾ï¼Œæƒé‡ç”±å¤–éƒ¨ lmbd_ortho æ§åˆ¶ã€‚
    """
    if z_shared is None and z_private is None:
        return torch.tensor(0.0)
    if z_shared is None:
        return torch.tensor(0.0, device=z_private.device)
    if z_private is None:
        return torch.tensor(0.0, device=z_shared.device)
    zs = F.normalize(z_shared, dim=-1)
    zp = F.normalize(z_private, dim=-1)
    # è‹¥ batch å°ºå¯¸ä¸ä¸€è‡´ï¼ŒæŒ‰æœ€å°å¯¹é½ï¼ˆç†è®ºä¸Šåº”ä¸€è‡´ï¼‰
    if zs.size(0) != zp.size(0):
        n = min(zs.size(0), zp.size(0))
        zs = zs[:n]
        zp = zp[:n]
    
    # ç¡®ä¿ç»´åº¦ä¸€è‡´æ‰èƒ½åšç‚¹ç§¯
    if zs.size(1) != zp.size(1):
        min_d = min(zs.size(1), zp.size(1))
        zs = zs[:, :min_d]
        zp = zp[:, :min_d]
        
    cos = (zs * zp).sum(dim=-1)
    return (cos ** 2).mean()


# ==========================
# è¯„ä¼°
# ==========================
@torch.no_grad()
def evaluate(model: nn.Module, loader: DataLoader, device: str, use_amp: bool = True) -> float:
    model.eval(); correct=total=0
    amp_dev = 'cuda' if (str(device).startswith('cuda') and torch.cuda.is_available()) else 'cpu'
    use_amp_flag = bool(use_amp and amp_dev == 'cuda')
    for batch in tqdm(loader, desc="Evaluating"):
        batch = batch.to(device)
        with autocast(device_type=amp_dev, enabled=use_amp_flag):
            out = model(batch)
            logits = out[0] if isinstance(out, (tuple, list)) else out
            pred = logits.argmax(dim=1)
        correct += (pred == batch.y).sum().item(); total += batch.num_graphs
    return correct / max(1, total)


# ==========================
# æŸå¤±è§£æï¼ˆå…¼å®¹ä¸¤å¥—å‚æ•°ï¼‰
# ==========================
def resolve_cls_loss(
    args,
    labels_all: List[int],
    train_idx: np.ndarray,
    num_classes: int,
    device: str
) -> Tuple[nn.Module, str]:
    # æ–°ç‰ˆå‚æ•°ï¼ˆä¸ train.py å¯¹é½ï¼‰
    loss_type = getattr(args, 'loss_type', None)  # {'ce','focal','cb_ce','cb_focal'}
    gamma = float(getattr(args, 'gamma', 2.0))
    beta  = float(getattr(args, 'beta', 0.999))

    # å…¼å®¹æ—§å‚æ•°
    use_focal    = bool(getattr(args, 'use_focal', False))
    focal_gamma  = float(getattr(args, 'focal_gamma', gamma))
    cb_beta      = float(getattr(args, 'cb_beta', beta))

    # ä»¥ loss_type ä¸ºå‡†ï¼›è‹¥æœªæä¾›åˆ™å›é€€åˆ°æ—§å‚æ•°é£æ ¼
    if loss_type is None:
        if use_focal and cb_beta is not None:
            loss_type = 'cb_focal'
        elif use_focal:
            loss_type = 'focal'
        elif cb_beta is not None:
            loss_type = 'cb_ce'
        else:
            loss_type = 'ce'

    # ç±»å¹³è¡¡æƒé‡
    weights = None
    if loss_type in ('cb_ce', 'cb_focal'):
        weights = class_balanced_weights(
            [labels_all[i] for i in train_idx], num_classes, beta=cb_beta
        ).to(device)

    # å…·ä½“æŸå¤±
    if loss_type in ('focal', 'cb_focal'):
        criterion = FocalLoss(gamma=focal_gamma, weight=weights)
        name = 'CB-Focal' if weights is not None else f'Focal(gamma={focal_gamma})'
    else:
        criterion = nn.CrossEntropyLoss(weight=weights)
        name = 'CB-CE' if weights is not None else 'CE'
    return criterion, name


# ==========================
# è®­ç»ƒä¸»å¾ªç¯
# ==========================
def train_loop(args):
    # ---------- Early-Stopping ----------
    patience = int(getattr(args, "patience", 10))  # é»˜è®¤ 10
    no_improve = 0

    # [1] ç§å­ & ç›®å½•
    set_seed(args.seed)
    os.makedirs(args.save_dir, exist_ok=True)
    best_path = os.path.join(args.save_dir, f"{args.exp_name}_best.pt")
    last_path = os.path.join(args.save_dir, f"{args.exp_name}_last.pt")

    # [2] æ•°æ®é›†
    print("[2] å¼€å§‹åŠ è½½æ•°æ®é›† (HeteroGraph, æ”¯æŒ NPS/TFF).")
    print(f"[Dataset] using module file: {ds.__file__}")
    sig_ds = inspect.signature(ds.MultiModalHeteroDatasetV2.__init__)
    allowed_ds = set(sig_ds.parameters.keys()) - {"self"}
    candidate_ds = dict(
        dataset=getattr(args, 'dataset', 'auto'),
        op_dir=args.op_dir, dose_dir=getattr(args, 'dose_dir', ''),
        op_interval=args.op_interval, dose_interval=args.dose_interval,
        dose_window_steps=args.dose_window_steps, dose_stride_steps=args.dose_stride_steps,
        task_type=args.task_type, clip_val=args.clip_val,
        op_topk=getattr(args,'op_topk',10), dose_topk=getattr(args,'dose_topk',10),
        min_abs_corr=getattr(args,'min_abs_corr',0.2),
        max_lag_dose=getattr(args,'max_lag_dose',5),
        topk_cross=getattr(args,'topk_cross',3),
        downsample_mode=getattr(args,'downsample_mode','pick'),
    )
    init_kwargs_ds = {k:v for k,v in candidate_ds.items() if k in allowed_ds}
    print(f"[Dataset] init kwargs: {sorted(init_kwargs_ds.keys())}")
    dataset = ds.MultiModalHeteroDatasetV2(**init_kwargs_ds)

    if len(dataset) == 0:
        raise RuntimeError(
            "[Dataset] æ„é€ å‡ºçš„çª—å£æ•°ä¸º 0ã€‚è¯·æ£€æŸ¥ï¼š\n"
            "  - --op_dir æ˜¯å¦æŒ‡å‘ NPS çš„ Operation CSV ç›®å½•\n"
            "  - --dose_dir æ˜¯å¦æœ‰ CSVï¼›è‹¥æ²¡æœ‰ï¼Œå½“å‰ç‰ˆæœ¬ä¼šç”¨ OP ä¸‹é‡‡æ ·ç”Ÿæˆ pseudo-dose\n"
            "  - --dose_window_steps æ˜¯å¦è¿‡å¤§ï¼ˆå•æ¡ DOSE åºåˆ—ä¸è¶³ä»¥æ»‘çª—ï¼‰\n"
            "  - CSV æ˜¯å¦åŒ…å«æ•°å€¼åˆ—ï¼ˆéæ•°å€¼ä¼šè¢«è¿‡æ»¤ï¼‰\n"
        )

    # [3] num_classes æ¨æ–­
    labels_all = getattr(dataset, "_win_labels", [])
    if args.task_type == 'binary':
        num_classes = 2
    elif getattr(dataset, "label2id", None):
        num_classes = len(dataset.label2id)
    elif labels_all:
        num_classes = int(max(labels_all)) + 1
    else:
        raise RuntimeError("[Dataset] æ— æ³•æ¨æ–­ç±»åˆ«æ•°ã€‚")

    # [4] åˆ’åˆ†
    labels_all: List[int] = getattr(dataset, '_win_labels', [])
    print(f"[DatasetV2] type={getattr(dataset,'dataset_type','?')} | windows={len(dataset)} "
          f"| dose_steps={dataset.dose_steps} | op_steps={dataset.op_steps} "
          f"| ratio={dataset.dose_interval//dataset.op_interval} | N_classes={num_classes}")
    cnt_all = Counter(labels_all)
    print("å…¨æ•°æ®é›†åˆ†å¸ƒï¼š")
    for c in sorted(cnt_all.keys()): print(f"  ç±»åˆ« {c:2d}: {cnt_all[c]}")
    # =============================
    # ç›¸é‚»æ ·æœ¬ä¸è½åŒä¸€å­é›†ï¼ˆçª—å£çº§äº¤é”™åˆ†é…ï¼Œåˆ†å¸ƒä¸€è‡´ï¼‰
    # =============================
    window2file = getattr(dataset, '_window2file', [])
    file_labels = getattr(dataset, '_labels', [])
    if not window2file or not file_labels:
        raise RuntimeError('[Split] ç¼ºå°‘æ–‡ä»¶çº§æ˜ å°„(_window2file/_labels)ï¼Œæ— æ³•è¿›è¡Œæ— é‡å åˆ’åˆ†ã€‚')

    # äº¤é”™åˆ†é…ï¼šå¯¹æ¯ä¸ªæ–‡ä»¶çš„çª—å£æŒ‰æ—¶é—´é¡ºåºäº¤é”™åˆ° train/val/testï¼Œç¡®ä¿ç›¸é‚»ä¸åœ¨åŒä¸€å­é›†
    files = np.arange(len(file_labels))
    data_fraction = float(getattr(args, 'data_fraction', 1.0))
    base_files = files
    if 0.0 < data_fraction < 1.0:
        # å­é›†æ–‡ä»¶é€‰æ‹©ä¸åˆ†å±‚ï¼Œé¿å…ç¨€æœ‰ç±»å¯¼è‡´æŠ¥é”™ï¼›åˆ†å¸ƒä¸€è‡´é€šè¿‡çª—å£çº§äº¤é”™å®ç°
        base_files, _ = train_test_split(files, train_size=data_fraction, random_state=args.seed, stratify=None)
        print(f"[Subset-Files] ä½¿ç”¨ {len(base_files)}/{len(files)} ä¸ªæ–‡ä»¶ (~{data_fraction*100:.1f}%).")

    # æ¨¡å¼ï¼šä¸¥æ ¼ç›¸é‚»ä¸åŒå­é›† + æ¯”ä¾‹ 7:1.5:1.5ï¼ˆçº¦ç­‰äº 0.7/0.15/0.15ï¼‰
    ratios = {'train': 0.7, 'val': 0.15, 'test': 0.15}
    subsets = ['train', 'val', 'test']
    train_idx, val_idx, test_idx = [], [], []
    # å…¨å±€ç›®æ ‡é…é¢ï¼Œé¿å…â€œæ¯æ–‡ä»¶å››èˆäº”å…¥â€é€ æˆç´¯è®¡è¯¯å·®
    all_wins = [w for w in range(len(window2file)) if window2file[w] in set(base_files)]
    total_n = len(all_wins)
    global_target = {
        'train': int(round(total_n * ratios['train'])),
        'val':   int(round(total_n * ratios['val'])),
        'test':  int(round(total_n * ratios['test']))
    }
    # è°ƒæ•´æ€»å’Œåˆ° total_n
    gdiff = total_n - sum(global_target.values())
    if gdiff != 0:
        for s in sorted(subsets, key=lambda k: ratios[k], reverse=True):
            if gdiff == 0: break
            global_target[s] += 1 if gdiff > 0 else -1
            gdiff += -1 if gdiff > 0 else 1
    global_used = {s: 0 for s in subsets}
    # éå†æ–‡ä»¶ï¼Œå†…éƒ¨ä»ä¿æŒç›¸é‚»ä¸åŒï¼Œä¼˜å…ˆæ»¡è¶³å…¨å±€å‰©ä½™é…é¢
    for f in base_files:
        wins = [w for w, fid in enumerate(window2file) if fid == f]
        prev = None
        for w in wins:
            # å€™é€‰æŒ‰å…¨å±€å‰©ä½™é…é¢æ’åºï¼Œä¸”ä¸ç­‰äº prev
            candidates = sorted(subsets, key=lambda s: (global_target[s] - global_used[s], ratios[s]), reverse=True)
            chosen = None
            for s in candidates:
                if s == prev: continue
                if global_used[s] < global_target[s]:
                    chosen = s
                    break
            if chosen is None:
                # è‹¥æ‰€æœ‰å€™é€‰å·²æ»¡æˆ–ä¸ prev å†²çªï¼Œé€‰æ‹©ä¸ prev ä¸åŒçš„é›†åˆï¼ˆå…è®¸ç•¥è¶…é…é¢ï¼‰
                for s in subsets:
                    if s != prev:
                        chosen = s
                        break
            global_used[chosen] += 1
            prev = chosen
            if chosen == 'train':
                train_idx.append(w)
            elif chosen == 'val':
                val_idx.append(w)
            else:
                test_idx.append(w)

    print('[4] çª—å£çº§äº¤é”™åˆ’åˆ† (ç›¸é‚»ä¸åŒè¡Œ, è¿‘ä¼¼å‡è¡¡).')
    print(f"çª—å£æ•°: train={len(train_idx)} val={len(val_idx)} test={len(test_idx)}")

    cnt_tr = Counter([labels_all[i] for i in train_idx])
    cnt_val = Counter([labels_all[i] for i in val_idx])
    cnt_te  = Counter([labels_all[i] for i in test_idx])
    print('è®­ç»ƒé›†åˆ†å¸ƒï¼š')
    for c in sorted(cnt_tr.keys()): print(f"  ç±»åˆ« {c:2d}: {cnt_tr[c]}")
    print('éªŒè¯é›†åˆ†å¸ƒï¼š')
    for c in sorted(cnt_val.keys()): print(f"  ç±»åˆ« {c:2d}: {cnt_val[c]}")
    print('æµ‹è¯•é›†åˆ†å¸ƒï¼š')
    for c in sorted(cnt_te.keys()): print(f"  ç±»åˆ« {c:2d}: {cnt_te[c]}")

    # [5] DataLoader
    print("[5] æ„å»º DataLoader.")
    train_set = Subset(dataset, train_idx)
    val_set   = Subset(dataset, val_idx)
    test_set  = Subset(dataset, test_idx)
    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True,
                              num_workers=getattr(args,'num_workers',8),
                              pin_memory=True, persistent_workers=False)
    val_loader   = DataLoader(val_set, batch_size=args.batch_size, shuffle=False,
                              num_workers=getattr(args,'test_workers',0),
                              pin_memory=True, persistent_workers=False)
    test_loader  = DataLoader(test_set,  batch_size=args.batch_size, shuffle=False,
                              num_workers=getattr(args,'test_workers',0),
                              pin_memory=True, persistent_workers=False)
    print(f"[5] DataLoader æ„å»ºå®Œæˆã€‚Train nw={getattr(args,'num_workers',8)} | Val nw={getattr(args,'test_workers',0)} | Test nw={getattr(args,'test_workers',0)}")

    # [6] åˆå§‹åŒ–æ¨¡å‹ â€”â€” è‡ªåŠ¨åˆ«åæ˜ å°„ï¼ˆå«å¿…éœ€çš„ seq_len/é€šé“æ•°/edge_attr_dimï¼‰
    print("[6] åˆå§‹åŒ–æ¨¡å‹ (HeteroGraph + edge_attr + Contrastive heads).")
    sig_m = inspect.signature(TemporalPhysicalHeteroGNN_V2.__init__)
    allowed_m = set(sig_m.parameters.keys()) - {"self"}
    cand_m = {
        # transformer/gnn ç›¸å…³ï¼ˆä¸ train.py å¯¹é½çš„å¸¸ç”¨å‘½åï¼‰
        'trans_dim'   : getattr(args,'trans_dim',256),
        'trans_layers': getattr(args,'trans_layers',2),
        'nhead'       : getattr(args,'nhead',4),
        'gcn_hidden'  : getattr(args,'gnn_hidden',getattr(args,'gcn_hidden',512)),
        'gcn_layers'  : getattr(args,'gnn_layers',getattr(args,'gcn_layers',2)),
        'dropout'     : getattr(args,'dropout',0.1),
        'num_classes' : num_classes,
        # æ¨¡æ€ç»´åº¦/åºåˆ—é•¿åº¦ï¼ˆæ¥è‡ªæ•°æ®é›†ï¼‰
        'num_op'      : int(getattr(dataset,'num_op',0)),
        'num_dose'    : int(getattr(dataset,'num_dose',0)),
        'op_seq_len'  : int(getattr(dataset,'op_steps',0)),
        'dose_seq_len': int(getattr(dataset,'dose_steps',0)),
        # è·¨æ¨¡æ€è¾¹ç‰¹å¾ç»´åº¦ï¼ˆæˆ‘ä»¬æ„é€ ä¸º 3ï¼‰
        'edge_attr_dim': 3,
    }
    alias = {
        'trans_dim'   : ['trans_dim','d_model','embed_dim','model_dim','hidden_dim'],
        'trans_layers': ['trans_layers','num_layers','encoder_layers','n_layers','layers'],
        'nhead'       : ['nhead','num_heads','heads','n_heads'],
        'gcn_hidden'  : ['gcn_hidden','gcn_hidden_dim','gcn_hidden_channels','hidden_channels','hidden_dim','gnn_hidden','gnn_dim'],
        'gcn_layers'  : ['gcn_layers','num_gcn_layers','gnn_layers','num_gnn_layers','num_conv_layers','num_layers_gnn'],
        'dropout'     : ['dropout','drop_rate','p_dropout'],
        'num_classes' : ['num_classes','n_classes','out_dim','out_channels','num_outputs'],
        'num_op'      : ['num_op','op_channels','in_dim_op','op_in_channels','op_nvars','n_op'],
        'num_dose'    : ['num_dose','dose_channels','in_dim_dose','dose_in_channels','dose_nvars','n_dose'],
        'op_seq_len'  : ['op_seq_len','seq_len_op','op_length','op_steps','len_op'],
        'dose_seq_len': ['dose_seq_len','seq_len_dose','dose_length','dose_steps','len_dose'],
        'edge_attr_dim': ['edge_attr_dim','edge_dim','edge_attr_size'],
    }
    used_map, init_kwargs_m = {}, {}
    for std_key, value in cand_m.items():
        for real_key in alias.get(std_key, [std_key]):
            if real_key in allowed_m:
                init_kwargs_m[real_key] = value
                used_map[std_key] = real_key
                break
    print(f"[Model] ctor allowed: {sorted(list(allowed_m))}")
    print(f"[Model] arg mapping: " + ", ".join([f"{k}->{v}" for k,v in used_map.items()]))
    missing_required = [p.name for p in sig_m.parameters.values()
                        if p.default is inspect._empty and p.name not in init_kwargs_m and p.name != 'self']
    if missing_required:
        raise TypeError(f"æ¨¡å‹æ„é€ ç¼ºå°‘å¿…è¦å‚æ•°: {missing_required}. å·²å¯ç”¨æ˜ å°„: {used_map}.")

    model = TemporalPhysicalHeteroGNN_V2(**init_kwargs_m).to(args.device)
    print("[6] æ¨¡å‹åˆå§‹åŒ–å®Œæˆã€‚")

    # [7] ä¸»æŸå¤± & ä¼˜åŒ–å™¨
    criterion_ce, ce_name = resolve_cls_loss(args, labels_all, train_idx, num_classes, args.device)
    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    # optimizer = SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)
    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

    # ---------- å¯¹æ¯”/ä¸€è‡´æ€§è¶…å‚ ----------
    lmbd_supcon = float(getattr(args,'lmbd_supcon',0.0))
    lmbd_xmod   = float(getattr(args,'lmbd_xmod',0.0))
    lmbd_aug    = float(getattr(args,'lmbd_aug',0.0))
    tau         = float(getattr(args,'tau',0.1))
    tau_aug     = float(getattr(args,'tau_aug',0.1))  # é¢„ç•™

    # ---------- æ–°å¢ï¼šæ­£äº¤ä¸ SoftCL è¶…å‚ ----------
    lmbd_ortho      = float(getattr(args, 'lmbd_ortho', 0.0))
    use_soft_supcon = bool(getattr(args, 'use_soft_supcon', False))
    soft_label_tau  = float(getattr(args, 'soft_label_tau', 0.5))
    shared_ratio    = float(getattr(args, 'shared_ratio', 0.5))

    # ---------- AMP / æ¢¯åº¦ç´¯ç§¯ ----------
    use_amp = bool(getattr(args,'use_amp',True))
    amp_dev = 'cuda' if (str(args.device).startswith('cuda') and torch.cuda.is_available()) else 'cpu'
    scaler = GradScaler(amp_dev, enabled=(use_amp and amp_dev=='cuda'))
    grad_accum = int(getattr(args,'grad_accum_steps', getattr(args,'grad_accum',1)))
    assert grad_accum >= 1

    # ---------- å¯é€‰ï¼šæ–­ç‚¹æ¢å¤ ----------
    start_epoch = 1  # ğŸ”µ èµ·å§‹ epoch
    no_improve = 0  # ğŸ”µ æ—©åœè®¡æ•°
    best_acc = -1.0

    if getattr(args, 'resume', '') and os.path.isfile(args.resume):
        try:
            # PyTorch 2.6 é»˜è®¤ weights_only=True ä¼šé˜»æ­¢æ—§ç‰ˆ checkpointï¼ˆå«ä¼˜åŒ–å™¨/è°ƒåº¦å™¨çŠ¶æ€ï¼‰æ¢å¤ã€‚
            # è¿™é‡Œæ˜¾å¼ç¦ç”¨è¯¥é™åˆ¶ï¼Œå‰ææ˜¯æˆ‘ä»¬ä¿¡ä»»æœ¬åœ°ä¿å­˜çš„ ckpt æ–‡ä»¶ã€‚
            state = torch.load(args.resume, map_location='cpu', weights_only=False)
            model.load_state_dict(state['model'], strict=True)
            optimizer.load_state_dict(state['optimizer'])
            if use_amp and state.get('scaler'):
                scaler.load_state_dict(state['scaler'])
            if 'scheduler' in locals() and state.get('scheduler'):
                scheduler.load_state_dict(state['scheduler'])
            best_acc = float(state.get('best_acc', -1.0))
            no_improve = int(state.get('no_improve', 0))
            start_epoch = int(state.get('epoch', 0)) + 1  # ğŸ”µ æ–°
            print(f"[*] å·²ä» {args.resume} æ¢å¤ï¼šç»§ç»­ epoch {start_epoch}ï¼Œbest_acc={best_acc:.4f}")

        except Exception as e:
            print(f"[WARN] æ¢å¤å¤±è´¥ï¼š{e}")

    # [8] è®­ç»ƒ
    print(f"[8] å¼€å§‹è®­ç»ƒä¸»å¾ªç¯ï¼ˆ{ce_name}+SupCon+XMod+Aug | AMP ä¸æ¢¯åº¦ç´¯ç§¯ï¼‰.")
    warned_no_embed = False
    warned_missing_xmod = False

    for epoch in range(start_epoch, args.epochs+1):
        model.train()
        epoch_loss=ce_sum=sup_sum=xmd_sum=aug_sum=ortho_sum=0.0; step_count=0
        pbar = tqdm(train_loader, total=len(train_loader), desc=f"Epoch {epoch:03d}")
        optimizer.zero_grad(set_to_none=True)

        for step, batch in enumerate(pbar, start=1):
            batch = batch.to(args.device)
            with autocast(device_type=amp_dev, enabled=(use_amp and amp_dev=='cuda')):
                out = model(batch)
                if isinstance(out, (tuple,list)):
                    logits = out[0]
                    z_fused = out[1] if len(out)>1 else None
                    z_op    = out[2] if len(out)>2 else None
                    z_dose  = out[3] if len(out)>3 else None
                else:
                    logits, z_fused, z_op, z_dose = out, None, None, None

                # é¦–ä¸ª batch æ‰“å°åµŒå…¥å½¢çŠ¶ï¼ˆä¾¿äºæ’æŸ¥ï¼‰
                if epoch == 1 and step == 1:
                    def shp(x): return None if x is None else tuple(x.shape)
                    print(f"[Probe] use_aug={getattr(args,'use_aug',True)} | "
                          f"z_fused={shp(z_fused)}, z_op={shp(z_op)}, z_dose={shp(z_dose)}")

                # -------------- shared/private åˆ†è§£ --------------
                z_shared = None
                z_op_sh = z_op_pr = None
                z_dose_sh = z_dose_pr = None

                can_split = (z_op is not None) and (z_dose is not None)
                if can_split:
                    d_op = z_op.size(-1)
                    d_ds = z_dose.size(-1)
                    if (d_op == d_ds):
                        # å…±äº«:ç§æœ‰ æŒ‰ shared_ratio åˆ‡åˆ†ï¼Œè½åœ¨ [1, d-1]
                        h = int(d_op * shared_ratio)
                        h = max(1, min(d_op - 1, h))
                        if h <= 0 or h >= d_op:
                            h = d_op // 2  # å…œåº•å›é€€ä¸ºå¯¹åŠåˆ†
                        z_op_sh, z_op_pr = z_op[:, :h], z_op[:, h:]
                        z_dose_sh, z_dose_pr = z_dose[:, :h], z_dose[:, h:]
                        z_shared = 0.5 * (z_op_sh + z_dose_sh)
                    else:
                        # ç»´åº¦ä¸åˆé€‚ï¼Œé€€åŒ–ä½¿ç”¨ z_fused ä½œä¸º shared
                        z_shared = z_fused if z_fused is not None else None
                else:
                    # ç¼ºå°‘æŸä¸€æ¨¡æ€ï¼Œé€€åŒ–ä½¿ç”¨ z_fused ä½œä¸º shared
                    z_shared = z_fused if z_fused is not None else None

                ce = criterion_ce(logits, batch.y)

                # ä¸‰ä¸ªé™„åŠ æŸå¤±
                sup = torch.tensor(0.0, device=args.device)
                xmd = torch.tensor(0.0, device=args.device)
                aug = torch.tensor(0.0, device=args.device)
                ortho = torch.tensor(0.0, device=args.device)

                # åµŒå…¥ç¼ºå¤±ä¸€æ¬¡æ€§å‘Šè­¦
                if (lmbd_supcon>0 or lmbd_xmod>0 or lmbd_aug>0) and (z_fused is None and z_op is None and z_dose is None) and (not warned_no_embed):
                    print("[WARN] æ¨¡å‹ forward æœªè¿”å›åµŒå…¥ï¼ˆz_fused/z_op/z_doseï¼‰ï¼Œå¯¹æ¯”/ä¸€è‡´æ€§æŸå¤±å°†è¢«è·³è¿‡ã€‚")
                    warned_no_embed=True

                # SupConï¼ˆä¼˜å…ˆä½¿ç”¨ sharedï¼Œæ”¯æŒ SoftCLï¼‰
                if lmbd_supcon>0 and z_shared is not None:
                    sup = supcon_loss(z_shared, batch.y, tau=tau,
                                      soft=use_soft_supcon, label_tau=soft_label_tau) * lmbd_supcon

                # XModï¼ˆä¼˜å…ˆå¯¹ shared å­ç©ºé—´å¯¹é½ï¼Œé€€åŒ–åˆ°åŸ z_op/z_doseï¼‰
                if lmbd_xmod>0:
                    if (z_op_sh is not None) and (z_dose_sh is not None):
                        xmd = cosine_align_loss(z_op_sh, z_dose_sh) * lmbd_xmod
                    elif (z_op is not None) and (z_dose is not None):
                        xmd = cosine_align_loss(z_op, z_dose) * lmbd_xmod
                    elif not warned_missing_xmod:
                        print("[WARN] XMD å¯ç”¨ä½†ç¼ºå°‘å¯å¯¹é½çš„è¡¨ç¤ºï¼ˆshared æˆ–åŸå§‹ z_op/z_doseï¼‰ï¼Œå·²è·³è¿‡è·¨æ¨¡æ€ä¸€è‡´æ€§ï¼›è¯·æ£€æŸ¥æ¨¡å‹ forward çš„è¿”å›ã€‚")
                        warned_missing_xmod = True

                # AUGï¼ˆè½»é‡ä¸€è‡´æ€§ï¼›é»˜è®¤ Dropoutï¼Œä¹Ÿå¯åˆ‡åˆ°å™ªå£°ï¼‰
                if lmbd_aug>0 and getattr(args, 'use_aug', True) and (z_fused is not None):
                    mode = str(getattr(args, 'aug_mode', 'dropout')).lower()
                    if mode == 'noise':
                        sigma = float(getattr(args, 'aug_noise_std', 0.05))
                        z_aug = z_fused + torch.randn_like(z_fused) * sigma
                    else:
                        p = float(getattr(args, 'aug_dropout', 0.2))
                        z_aug = F.dropout(z_fused, p=p, training=True)

                    za = F.normalize(z_fused, dim=-1)
                    zb = F.normalize(z_aug,   dim=-1)
                    # ç”¨åŒä¸€é£æ ¼çš„â€œ1-cosâ€å¯¹é½ï¼›å¦‚éœ€æ¸©åº¦ï¼Œå¯æ›¿æ¢ä¸º (1 - cos/Ï„_aug) ç­‰è‡ªå®šä¹‰å½¢å¼
                    aug = (1.0 - (za * zb).sum(dim=-1)).mean() * lmbd_aug

                # æ­£äº¤æŸå¤±ï¼ˆéœ€è¦æˆåŠŸæ‹†åˆ†å‡º private å­ç©ºé—´ï¼‰
                if lmbd_ortho > 0:
                    o_sum = torch.tensor(0.0, device=args.device)
                    cnt = 0
                    if (z_op_sh is not None) and (z_op_pr is not None):
                        o_sum = o_sum + ortho_penalty(z_op_sh, z_op_pr)
                        cnt += 1
                    if (z_dose_sh is not None) and (z_dose_pr is not None):
                        o_sum = o_sum + ortho_penalty(z_dose_sh, z_dose_pr)
                        cnt += 1
                    if cnt > 0:
                        ortho = o_sum * lmbd_ortho

                loss = (ce + sup + xmd + aug + ortho) / grad_accum

            # åä¼ ä¸ä¼˜åŒ–
            if use_amp and amp_dev=='cuda':
                scaler.scale(loss).backward()
                if step % grad_accum == 0:
                    scaler.step(optimizer); scaler.update(); optimizer.zero_grad(set_to_none=True)
            else:
                loss.backward()
                if step % grad_accum == 0:
                    optimizer.step(); optimizer.zero_grad(set_to_none=True)

            # ç»Ÿè®¡
            epoch_loss += float((loss * grad_accum).detach().cpu())
            ce_sum += float((ce).detach().cpu())
            sup_sum += float((sup).detach().cpu())
            xmd_sum += float((xmd).detach().cpu())
            aug_sum += float((aug).detach().cpu())
            ortho_sum += float((ortho).detach().cpu())
            step_count += 1
            pbar.set_postfix(loss=(loss.item()*grad_accum),
                             ce=ce.item(), sup=float(sup), xmd=float(xmd), aug=float(aug), ortho=float(ortho))

        # [9] è¯„ä¼°
        acc = evaluate(model, test_loader, args.device, use_amp=use_amp)
        print(f"[Epoch {epoch:03d}] TrainLoss:{epoch_loss/max(1,step_count):.4f} | CE:{ce_sum/max(1,step_count):.4f} "
              f"Sup:{sup_sum/max(1,step_count):.4f} XMD:{xmd_sum/max(1,step_count):.4f} AUG:{aug_sum/max(1,step_count):.4f} "
              f"ORTH:{ortho_sum/max(1,step_count):.4f} | "
              f"Test Acc:{acc:.4f} | Best:{(-1.0 if epoch==1 and best_acc<0 else best_acc):.4f}")

        # [10] ä¿å­˜
        state = {'epoch': epoch,
                 'model': model.state_dict(),
                 'optimizer': optimizer.state_dict(),
                 'args': vars(args),
                 'train_idx': train_idx, 'test_idx': test_idx,
                 'best_acc': max(best_acc, acc)}
        torch.save(state, last_path)

        if acc > best_acc:  # æœ‰æå‡ âœ è®°å½• & å½’é›¶è®¡æ•°
            best_acc = acc
            no_improve = 0
            torch.save(state, best_path)
            print(f"[*] æ–°æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {best_path}")
            print(f"Test Acc improved -> {best_acc:.4f}")
        else:  # æ— æå‡ âœ è®¡æ•° +1
            no_improve += 1
            print(f"[EarlyStop] no_improve = {no_improve}/{patience}")

        # -------- è§¦å‘ Early-Stopping --------
        if no_improve >= patience:
            print(f"[EarlyStop] è¿ç»­ {patience} ä¸ª epoch æœªæå‡ï¼Œæå‰ç»ˆæ­¢è®­ç»ƒã€‚")
            break
